now how do I train my lora?

The way I did it manually was i went to https://www.runpod.io/console/fine-tuning
and then filled in the details wuith the base model (from .env.local) and the hf token (from .env.local) and the dataset {dataset_repo_id} (from the code below) and then clicked deploy fine tuning pod.
Then ... I chose a GPU: I clicked on the A40 GPU $0.4/hr 48 GB VRAM 48 GB RAM 9vCPU GPU
after this ... I named my pod (doesnt rly matter what you name it) and clicked the on-demand non-interruptible $0.40/hr pay as you go instance pricing option. 
Finally, I clicked 'Deploy on-demand' button ... I waited a bit while my pod was being built and then when it was it redirected me to my pods dashboard
After this, I waited some more until the logs said that that it was configured and ready to train. So then i clicked the button "Connect" and clicked 'start' on the web terminal option, which gave me a link to open the web terminal (and I did).
Then I executed this command in the web terminal (as this file was premade with the pod): 'nano config.yaml'
and I deleted this part (note that instead of avai-hf you should think ofthis as the env variable and dependant on the HF_USERNAME (lora dataset path is from {dataset_repo_id})):

datasets:
    - message_property_mappings:
      content: content
      role: role
      path: avai-hf/1ae5733c-1ce8-4bcb-b600-5872108e2c0a-dataset
      trust_remote_code: false

and replaced it with this:

datasets:
  - path: avai-hf/1ae5733c-1ce8-4bcb-b600-5872108e2c0a-dataset
    type: completion
    field: text
    trust_remote_code: false

then i confirmed this change (manually I pressed Ctrl + X to go out and then it said do you want to save and i entered 'Y' and then it asked me what is the filename (and the default one was config.yaml) so i pressed enter).
After, when config.yaml was updated (explained above), I stated training by executing the comand (typed): axolotl train config.yaml

--------------------------------------------

Once training was finished:

1. somehow sign in to hf

2. Create a repo on hf for the model. Example:
from huggingface_hub import create_repo
create_repo("lora1-model", private=True)

3. Upload the lora model to hf. In the web terminal cli I ran huggingface-cli upload {myusername}/{name of model (example: lora1-model)} {here you put the output folder; the thing you read on the right side of the ':' in the 'output_dir' parameter of config.yaml } 

--------------------------------------------------------------------------

After uploading the model to hugging face and confirming it exists under the models in my hf, I then closed my web terminal, went to run pod where it said web terminal is running or smth, stopped the web terminal, stopped the pod and then clicked 'terminate' button on the pod to delete it completely.





--------------------------------------------------------------------------

Not sure about:

<delete_repo>
( repo_id: strtoken: Union[str, bool, None] = Nonerepo_type: Optional[str] = Nonemissing_ok: bool = False )

Parameters

repo_id (str) — A namespace (user or an organization) and a repo name separated by a /.
token (Union[bool, str, None], optional) — A valid user access token (string). Defaults to the locally saved token, which is the recommended method for authentication (see https://huggingface.co/docs/huggingface_hub/quick-start#authentication). To disable authentication, pass False.
repo_type (str, optional) — Set to "dataset" or "space" if uploading to a dataset or space, None or "model" if uploading to a model.
missing_ok (bool, optional, defaults to False) — If True, do not raise an error if repo does not exist.
Raises RepositoryNotFoundError