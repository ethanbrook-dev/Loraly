now how do I train my lora?

The way I did it manually was i went to https://www.runpod.io/console/fine-tuning
and then filled in the details wuith the base model (from .env.local) and the hf token (from .env.local) and the dataset {dataset_repo_id} (from the code below) and then clicked deploy fine tuning pod.
Then ... I chose a GPU: I clicked on the A40 GPU $0.4/hr 48 GB VRAM 48 GB RAM 9vCPU GPU
after this ... I named my pod (doesnt rly matter what you name it) and clicked the on-demand non-interruptible $0.40/hr pay as you go instance pricing option. 
Finally, I clicked 'Deploy on-demand' button ... I waited a bit while my pod was being built and then when it was it redirected me to my pods dashboard
After this, I waited some more until the logs said that that it was configured and ready to train. So then i clicked the button "Connect" and clicked 'start' on the web terminal option, which gave me a link to open the web terminal (and I did).
Then I executed this command in the web terminal (as this file was premade with the pod): 'nano config.yaml'
and I deleted this part (note that instead of avai-hf you should think ofthis as the env variable and dependant on the HF_USERNAME (lora dataset path is from {dataset_repo_id})):

datasets:
    - message_property_mappings:
      content: content
      role: role
      path: avai-hf/1ae5733c-1ce8-4bcb-b600-5872108e2c0a-dataset
      trust_remote_code: false

and replaced it with this:

datasets:
  - path: avai-hf/1ae5733c-1ce8-4bcb-b600-5872108e2c0a-dataset
    type: completion
    field: text
    trust_remote_code: false

then i confirmed this change (manually I pressed Ctrl + X to go out and then it said do you want to save and i entered 'Y' and then it asked me what is the filename (and the default one was config.yaml) so i pressed enter).
After, when config.yaml was updated (explained above), I stated training by executing the comand (typed): axolotl train config.yaml

--------------------------------------------

Once training was finished:

1. somehow sign in to hf

2. Create a repo on hf for the model. Example:
from huggingface_hub import create_repo
create_repo("lora1-model", private=True)

3. Upload the lora model to hf. In the web terminal cli I ran huggingface-cli upload {myusername}/{name of model (example: lora1-model)} {here you put the output folder; the thing you read on the right side of the ':' in the 'output_dir' parameter of config.yaml } 

--------------------------------------------------------------------------

After uploading the model to hugging face and confirming it exists under the models in my hf, I then closed my web terminal, went to run pod where it said web terminal is running or smth, stopped the web terminal, stopped the pod and then clicked 'terminate' button on the pod to delete it completely.



adapter: lora
base_model: mistralai/Mistral-7B-Instruct-v0.3
bf16: auto
dataset_processes: 32

datasets:
  - path: avai-hf/1ae5733c-1ce8-4bcb-b600-5872108e2c0a-dataset
    type: completion
    field: text
    trust_remote_code: false

gradient_accumulation_steps: 1
gradient_checkpointing: false
learning_rate: 0.0002
lisa_layers_attribute: model.layers
load_best_model_at_end: false
load_in_4bit: false
load_in_8bit: true
lora_alpha: 16
lora_dropout: 0.05
lora_r: 8
lora_target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  - gate_proj
  - down_proj
  - up_proj
loraplus_lr_embedding: 1.0e-06
lr_scheduler: cosine
max_prompt_len: 512
mean_resizing_embeddings: false
micro_batch_size: 16
num_epochs: 1.0
optimizer: adamw_bnb_8bit
output_dir: ./outputs/mymodel
pretrain_multipack_attn: true
pretrain_multipack_buffer_size: 10000
qlora_sharded_model_loading: false
ray_num_workers: 1
resources_per_worker:
  GPU: 1
sample_packing_bin_size: 200
sample_packing_group_size: 100000
save_only_model: false
save_safetensors: true
sequence_len: 4096
shuffle_merged_datasets: true
skip_prepare_dataset: false
strict: false
train_on_inputs: false
trl:
  log_completions: false
  ref_model_mixup_alpha: 0.9
  ref_model_sync_steps: 64
  sync_ref_model: false
  use_vllm: false
  vllm_device: auto
  vllm_dtype: auto
  vllm_gpu_memory_utilization: 0.9
use_ray: false
val_set_size: 0.0
weight_decay: 0.0

--------------------------------------------------------------------------

Not sure about:

<delete_repo>
( repo_id: strtoken: Union[str, bool, None] = Nonerepo_type: Optional[str] = Nonemissing_ok: bool = False )

Parameters

repo_id (str) — A namespace (user or an organization) and a repo name separated by a /.
token (Union[bool, str, None], optional) — A valid user access token (string). Defaults to the locally saved token, which is the recommended method for authentication (see https://huggingface.co/docs/huggingface_hub/quick-start#authentication). To disable authentication, pass False.
repo_type (str, optional) — Set to "dataset" or "space" if uploading to a dataset or space, None or "model" if uploading to a model.
missing_ok (bool, optional, defaults to False) — If True, do not raise an error if repo does not exist.
Raises RepositoryNotFoundError